
===== BEGIN FILE: Dockerfile =====

===== END FILE: Dockerfile =====

===== BEGIN FILE: app.py =====

===== END FILE: app.py =====

===== BEGIN FILE: main.py =====
from dataclasses import dataclass
from typing import Callable, Type, Any, Optional
from src.moneylion import logger

# Import your existing pipeline classes
from src.moneylion.pipeline.data_ingestion_pipeline import DataIngestionPipeline
from src.moneylion.pipeline.data_transformation_pipeline import DataTransformationPipeline
from src.moneylion.pipeline.data_preprocessing_pipeline import DataPreprocessingPipeline
from src.moneylion.pipeline.embedding_pipeline import EmbeddingPipeline
from src.moneylion.pipeline.model_training_pipeline import ModelTrainingPipeline

@dataclass(frozen=True)
class StageSpec:
    pipeline_cls: Type[Any]
    method_name: str
    require_true: bool = False  # set True for validation-like stages


class MainSequence:
    def __init__(self):
        # Define the sequence once; easy to reorder or extend
        self.stages: list[StageSpec] = [
            # StageSpec(DataIngestionPipeline, 'initiate_data_ingestion'),
            # StageSpec(DataTransformationPipeline, 'initiate_data_transformation'),
            # StageSpec(DataPreprocessingPipeline, 'initiate_data_preprocessing'),
            # StageSpec(EmbeddingPipeline, 'initiate_embedding'),
            StageSpec(ModelTrainingPipeline, 'initiate_model_training'),
        ]

    def _run_stage(self, spec: StageSpec) -> Optional[Any]:
        obj = spec.pipeline_cls()
        stage_name = getattr(obj, 'STAGE_NAME', obj.__class__.__name__)

        logger.info(f'>>>> stage {stage_name} started <<<<')
        result = getattr(obj, spec.method_name)()
        if spec.require_true and not result:
            # keep your original behavior for validation failure
            raise Exception('VALIDATION FAILED')
        logger.info(f'>>>> stage {stage_name} completed <<<<')
        return result

    def run(self) -> None:
        try:
            for spec in self.stages:
                self._run_stage(spec)
        except Exception as e:
            logger.exception(e)
            raise


if __name__ == '__main__':
    main_sequence = MainSequence()
    main_sequence.run()

===== END FILE: main.py =====

===== BEGIN FILE: requirements.txt =====
# =================================================================
# Core Data Science
# =================================================================
pandas
numpy
scikit-learn
torch


# =================================================================
# Utilities
# =================================================================
PyYAML
python-box
python-dotenv
ensure
tqdm


# =================================================================
# Google Cloud Platform (GCP) 
# =================================================================
google-cloud-storage

# =================================================================
# Experiment Tracking 
# =================================================================
mlflow
xgboost

# =================================================================
# Development & Visualization
# =================================================================
# notebook
# matplotlib
# seaborn

# =================================================================
# API Deployment (Optional for Stage 3)
# =================================================================
# Flask
# gunicorn


===== END FILE: requirements.txt =====

===== BEGIN FILE: setup.py =====

===== END FILE: setup.py =====

===== BEGIN FILE: src/moneylion/components/data_ingestion.py =====
import os
from pathlib import Path
from google.cloud import storage
from src.moneylion import logger
from src.moneylion.entity.config_entity import DataIngestionConfig
from src.moneylion.utils.common import create_directories


class DataIngestion:

    def __init__(self, config: DataIngestionConfig) -> None:
        self.config = config
        # Ensure the destination directory exists before any downloads
        create_directories([self.config.local_download_dir])
        self.storage_client = storage.Client()

    def download_files(self) -> None:
        # Get a client handle to the GCS bucket
        bucket = self.storage_client.bucket(self.config.gcs_bucket_name)
        logger.info(f"Successfully connected to GCS bucket: '{self.config.gcs_bucket_name}'")

        for filename in self.config.raw_files:
            local_path = os.path.join(self.config.local_download_dir, filename)
            blob_path = os.path.join(self.config.gcs_source_folder, filename)
            blob = bucket.blob(blob_path)
            blob.download_to_filename(local_path)
            logger.info(f"Successfully downloaded: '{filename}' from GCS to {local_path}")

===== END FILE: src/moneylion/components/data_ingestion.py =====

===== BEGIN FILE: src/moneylion/components/data_preprocessing.py =====
# src/moneylion/components/data_preprocessing.py

from __future__ import annotations
import json
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

from src.moneylion import logger
from src.moneylion.entity.config_entity import DataPreprocessingConfig
from src.moneylion.utils.common import create_directories


_SPECIAL_COLS: List[str] = [
    ".underwritingdataclarity.clearfraud.clearfraudidentityverification.nameaddressreasoncode",
    ".underwritingdataclarity.clearfraud.clearfraudidentityverification.ssnnamereasoncodedescription",
    ".underwritingdataclarity.clearfraud.clearfraudidentityverification.nameaddressreasoncodedescription",
    ".underwritingdataclarity.clearfraud.clearfraudidentityverification.phonetype",
    ".underwritingdataclarity.clearfraud.clearfraudidentityverification.ssndobreasoncode",
    ".underwritingdataclarity.clearfraud.clearfraudidentityverification.ssnnamereasoncode",
    ".underwritingdataclarity.clearfraud.clearfraudidentityverification.nameaddressreasoncode",
]

_ID_COLS = ["loanId", "underwritingid", "clarityFraudId"]
_LABEL_COL = "isBadDebt"


class DataPreprocessor:
    def __init__(self, config: DataPreprocessingConfig) -> None:
        self.cfg = config
        create_directories([self.cfg.root_dir])

    # ------------------------------------------------------------------ #
    # helpers
    # ------------------------------------------------------------------ #
    @staticmethod
    def _build_vocab(series: pd.Series) -> Dict[str, int]:
        uniq = sorted(series.unique().tolist())
        return {token: idx for idx, token in enumerate(uniq)}

    def _split_and_save(
        self,
        X_num: np.ndarray,
        X_cat: np.ndarray,
        y: np.ndarray,
    ) -> None:
        """train / val / test stratified split and npy dump"""
        train_idx, temp_idx = train_test_split(
            np.arange(len(y)),
            test_size=self.cfg.test_size + self.cfg.val_size,
            stratify=y,
            random_state=self.cfg.random_state,
        )
        rel_test = self.cfg.test_size / (self.cfg.test_size + self.cfg.val_size)
        val_idx, test_idx = train_test_split(
            temp_idx,
            test_size=rel_test,
            stratify=y[temp_idx],
            random_state=self.cfg.random_state,
        )

        splits = {
            "train": train_idx,
            "val": val_idx,
            "test": test_idx,
        }

        for split_name, idx in splits.items():
            np.save(self.cfg.root_dir / f"{split_name}_num.npy", X_num[idx].astype("float32"))
            np.save(self.cfg.root_dir / f"{split_name}_cat.npy", X_cat[idx].astype("int64"))
            np.save(self.cfg.root_dir / f"{split_name}_y.npy",   y[idx].astype("int64"))
            logger.info("Saved %s split – %d rows", split_name, len(idx))

    # ------------------------------------------------------------------ #
    # public entry
    # ------------------------------------------------------------------ #
    def run(self) -> None:
        logger.info("Loading joined_df from %s", self.cfg.joined_csv)
        df = pd.read_csv(self.cfg.joined_csv, low_memory=False)

        # ------------------------------------------------------------------
        # 1. Prepare categorical special columns
        # ------------------------------------------------------------------
        cat_idx_arrays: List[np.ndarray] = []
        vocabs_json: Dict[str, Dict[str, int]] = {}

        for col in _SPECIAL_COLS:
            if col not in df.columns:
                logger.warning("Special column %s not found – filling 'NA'", col)
                df[col] = "NA"
            df[col] = df[col].fillna("NA").astype(str)

            vocab = self._build_vocab(df[col])
            vocabs_json[col] = vocab
            cat_idx_arrays.append(df[col].map(vocab).to_numpy(np.int64))

        X_cat = np.stack(cat_idx_arrays, axis=1)  # shape [N, C]
        logger.info("Built categorical matrix shape %s", X_cat.shape)

        # ------------------------------------------------------------------
        # 2. Numeric matrix 
        # ------------------------------------------------------------------
        numeric_cols = [
            c
            for c in df.columns
            if c not in (_SPECIAL_COLS + _ID_COLS + [_LABEL_COL])
        ]
        X_num = df[numeric_cols].to_numpy(np.float32)
        logger.info("Numeric matrix shape %s", X_num.shape)

        # ------------------------------------------------------------------
        # 3. Labels
        # ------------------------------------------------------------------
        y = df[_LABEL_COL].to_numpy(np.int64)

        # ------------------------------------------------------------------
        # 4. Split and save
        # ------------------------------------------------------------------
        self._split_and_save(X_num, X_cat, y)

        # ------------------------------------------------------------------
        # 5. Persist metadata
        # ------------------------------------------------------------------
        with open(self.cfg.root_dir / "vocabs.json", "w") as f:
            json.dump(vocabs_json, f, indent=2)
        with open(self.cfg.root_dir / "columns.json", "w") as f:
            json.dump(
                {
                    "categorical": _SPECIAL_COLS,
                    "numeric": numeric_cols,
                    "label": _LABEL_COL,
                },
                f,
                indent=2,
            )

        logger.info("Data-preprocessing completed. Artifacts written to %s", self.cfg.root_dir)

===== END FILE: src/moneylion/components/data_preprocessing.py =====

===== BEGIN FILE: src/moneylion/components/data_transformation.py =====
# src/moneylion/components/data_transformation.py

import pandas as pd 
import numpy as np
import copy
from pathlib import Path

from src.moneylion import logger
from src.moneylion.entity.config_entity import DataTransformationConfig
from src.moneylion.utils.common import create_directories


class DataTransformation:
    def __init__(self, config: DataTransformationConfig) -> None:
        self.config = config
        create_directories([self.config.root_dir])

    @staticmethod
    def _calculate_z_score(series: pd.Series) -> pd.Series:
        mean = series.mean()
        std  = series.std(ddof=0) or 1.0  # avoid division by zero
        return (series - mean) / std

    @staticmethod
    def _contains_match(col: pd.Series) -> bool:
        return col.astype(str).str.lower().str.contains("match").any()

    def transform_data(self) -> None:

        loan_df = pd.read_csv(self.config.loan_raw)
        bad_status  = [
            'Charged Off', 'Settled Bankruptcy', 'Charged Off Paid Off',
            'External Collection', 'Internal Collection', 'Rejected'
        ]
        good_status = ['Paid Off Loan', 'Settlement Paid Off']

        loan_df['isBadDebt'] = loan_df['loanStatus'].apply(
            lambda x: True  if x in bad_status  else
                      False if x in good_status else None
        )

        # keep only rows we can label
        loan_df = loan_df[loan_df['loanStatus'].isin(bad_status + good_status)]

        # remove dirty rows 
        loan_df = loan_df[~((loan_df['isFunded'] == 1) & (loan_df['loanStatus'] == 'Rejected'))]
        loan_df = loan_df[~((loan_df['isFunded'] == 0) & (loan_df['loanStatus'] != 'Rejected'))]

        wanted_cols = [
            'loanId', 'payFrequency', 'apr', 'originated', 'nPaidOff',
            'isBadDebt', 'loanAmount',
            'originallyScheduledPaymentAmount', 'state', 'leadType',
            'fpStatus', 'clarityFraudId', 'hasCF'
        ]
        loan_df = loan_df[wanted_cols]

        # one-hot encode categorical cols
        dummies = pd.get_dummies(
            loan_df[['payFrequency', 'state', 'leadType', 'fpStatus']],
            prefix=['payFrequency', 'state', 'leadType', 'fpStatus'],
            dtype=int
        )
        loan_df = pd.concat(
            [loan_df.drop(columns=['payFrequency', 'state', 'leadType', 'fpStatus']),
             dummies],
            axis=1
        )

        # boolean → int
        loan_df['originated'] = loan_df['originated'].astype(int)
        loan_df['isBadDebt']  = loan_df['isBadDebt'].astype(int)

        # z-score selected numeric columns
        for col in ['apr', 'nPaidOff', 'loanAmount', 'originallyScheduledPaymentAmount']:
            loan_df[col] = self._calculate_z_score(loan_df[col])

        # 2 ─────────────────────────────── Clarity data
        clarity_df = pd.read_csv(self.config.clarity_raw, low_memory=False)

        num_cols     = clarity_df.select_dtypes(include=['number']).columns.tolist()
        bool_cols    = clarity_df.select_dtypes(include=['bool']).columns.tolist()
        match_cols   = [c for c in clarity_df.columns
                        if self._contains_match(clarity_df[c])]

        special_cols = [
            '.underwritingdataclarity.clearfraud.clearfraudidentityverification.nameaddressreasoncode',
            '.underwritingdataclarity.clearfraud.clearfraudidentityverification.ssnnamereasoncodedescription',
            '.underwritingdataclarity.clearfraud.clearfraudidentityverification.nameaddressreasoncodedescription',
            '.underwritingdataclarity.clearfraud.clearfraudidentityverification.phonetype',
            '.underwritingdataclarity.clearfraud.clearfraudidentityverification.ssndobreasoncode',
            '.underwritingdataclarity.clearfraud.clearfraudidentityverification.ssnnamereasoncode',
            '.underwritingdataclarity.clearfraud.clearfraudidentityverification.nameaddressreasoncode'
        ]

        keep_cols = list(
            set(num_cols + bool_cols + match_cols + special_cols + ['underwritingid'])
        )
        clarity_df = clarity_df[keep_cols]

        # deep copy for transformations
        clarity_tf = copy.deepcopy(clarity_df)

        # “match” cols → 0/1
        for col in match_cols:
            clarity_tf[col] = clarity_df[col].apply(
                lambda x: 1 if "match" in str(x).lower() else 0
            )

        # special presence cols → 0/1
        for col in special_cols:
            if col in clarity_tf.columns:
                clarity_tf[col] = clarity_df[col].apply(
                    lambda x: str(x) if pd.notnull(x) and x != '' else '0'
                )

        # bool → int
        for col in bool_cols:
            clarity_tf[col] = clarity_df[col].astype(int)

        # z-score numeric
        for col in num_cols:
            clarity_tf[col] = self._calculate_z_score(clarity_df[col])

        # Join
        loan_df['clarityFraudId']   = loan_df['clarityFraudId'].astype(str)
        clarity_tf['underwritingid'] = clarity_tf['underwritingid'].astype(str)

        joined_df = loan_df.merge(
            clarity_tf,
            how='left',
            left_on='clarityFraudId',
            right_on='underwritingid'
        ).fillna(0)

        # Save
        out_path = Path(self.config.joined_local)
        joined_df.to_csv(out_path, index=False)
        logger.info(f"joined_df created with shape {joined_df.shape} → {out_path}")

===== END FILE: src/moneylion/components/data_transformation.py =====

===== BEGIN FILE: src/moneylion/components/embedding_export.py =====
from __future__ import annotations
import json
import math
from pathlib import Path
from typing import Dict, List

import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.utils import shuffle

from src.moneylion import logger
from src.moneylion.entity.config_entity import DataEmbeddingConfig
from src.moneylion.utils.common import create_directories


class _EmbedNet(nn.Module):
    def __init__(
        self,
        cardinalities: List[int],
        numeric_dim: int,
        dim_rule: str = "sqrt",
    ) -> None:
        super().__init__()

        self.emb_layers = nn.ModuleList()
        emb_dim_total   = 0
        for card in cardinalities:
            if dim_rule.isdigit():
                emb_dim = int(dim_rule)
            else:                          # rule == "sqrt"
                emb_dim = int(math.sqrt(card))
            emb_dim_total += emb_dim
            self.emb_layers.append(nn.Embedding(card, emb_dim))

        self.numeric_bn = nn.BatchNorm1d(numeric_dim) if numeric_dim else nn.Identity()

        self.head = nn.Sequential(
            nn.Linear(emb_dim_total + numeric_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1)
        )

    def forward(self, cat_idx: torch.Tensor, num: torch.Tensor) -> torch.Tensor:
        emb_vecs = [emb(cat_idx[:, i]) for i, emb in enumerate(self.emb_layers)]
        concat   = torch.cat(emb_vecs + [num], dim=1)
        return self.head(concat), torch.cat(emb_vecs, dim=1)  # (logit, embed_vec)


class EmbeddingExporter:
    def __init__(self, cfg: DataEmbeddingConfig) -> None:
        self.cfg = cfg
        create_directories([self.cfg.root_dir])

        pp_dir = self.cfg.preproc_dir
        self.X_train_cat = np.load(pp_dir / "train_cat.npy")
        self.X_train_num = np.load(pp_dir / "train_num.npy")
        self.y_train     = np.load(pp_dir / "train_y.npy")

        self.X_val_cat   = np.load(pp_dir / "val_cat.npy")
        self.X_val_num   = np.load(pp_dir / "val_num.npy")
        self.y_val       = np.load(pp_dir / "val_y.npy")

        # vocabulary sizes
        with open(pp_dir / "vocabs.json") as f:
            self.vocabs: Dict[str, Dict[str, int]] = json.load(f)
        self.cardinalities = [len(stoi) for stoi in self.vocabs.values()]
        self.numeric_dim   = self.X_train_num.shape[1]

        # ---------- model ----------
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model  = _EmbedNet(
            self.cardinalities,
            self.numeric_dim,
            self.cfg.embedding_dim_rule
        ).to(self.device)

    # -------------------------------------------------------------- #
    def _dl(self, X_cat, X_num, y, shuffle_flag=True) -> DataLoader:
        Xc = torch.tensor(X_cat, dtype=torch.long)
        Xn = torch.tensor(X_num, dtype=torch.float32)
        yt = torch.tensor(y,     dtype=torch.float32).unsqueeze(1)
        ds = TensorDataset(Xc, Xn, yt)
        return DataLoader(
            ds,
            batch_size=self.cfg.batch_size,
            shuffle=shuffle_flag,
            num_workers=0,
            drop_last=False,
        )

    def train(self) -> None:
        torch.manual_seed(self.cfg.random_state)
        opt = torch.optim.Adam(self.model.parameters(), lr=self.cfg.lr)
        bce = nn.BCEWithLogitsLoss()

        train_loader = self._dl(self.X_train_cat, self.X_train_num, self.y_train)
        val_loader   = self._dl(self.X_val_cat,   self.X_val_num,   self.y_val, False)

        for epoch in range(self.cfg.epochs):
            self.model.train()
            epoch_loss = 0.0
            for Xc, Xn, y in train_loader:
                Xc, Xn, y = Xc.to(self.device), Xn.to(self.device), y.to(self.device)
                opt.zero_grad()
                logit, _ = self.model(Xc, Xn)
                loss = bce(logit, y)
                loss.backward()
                opt.step()
                epoch_loss += loss.item() * len(y)

            # simple val AUC / loss logging (optional)
            self.model.eval()
            with torch.no_grad():
                val_loss = 0.0
                for Xc, Xn, y in val_loader:
                    Xc, Xn, y = Xc.to(self.device), Xn.to(self.device), y.to(self.device)
                    logit, _  = self.model(Xc, Xn)
                    val_loss += bce(logit, y).item() * len(y)
            logger.info(
                f"Epoch {epoch+1}/{self.cfg.epochs} "
                f"train_loss={epoch_loss/len(self.y_train):.4f} "
                f"val_loss={val_loss/len(self.y_val):.4f}"
            )

    def _export_row_embeddings(self, X_cat: np.ndarray) -> np.ndarray:
        self.model.eval()
        loader = DataLoader(
            torch.tensor(X_cat, dtype=torch.long),
            batch_size=4096,
            shuffle=False
        )
        all_vecs = []
        with torch.no_grad():
            for Xc in loader:
                Xc = Xc.to(self.device)
                _, emb = self.model(Xc, torch.zeros((len(Xc), self.numeric_dim), device=self.device))
                all_vecs.append(emb.cpu().numpy())
        return np.vstack(all_vecs)

    def export(self) -> None:
        # Save raw embedding matrices per column (optional, nice for inspection)
        embed_dir = self.cfg.root_dir / "embed_matrices"
        create_directories([embed_dir])
        for name, emb_layer in zip(self.vocabs.keys(), self.model.emb_layers):
            np.save(embed_dir / f"{name}.npy", emb_layer.weight.detach().cpu().numpy())

        # Build dense matrices = [emb_vec | numeric]
        for split in ["train", "val", "test"]:
            X_cat = np.load(self.cfg.preproc_dir / f"{split}_cat.npy")
            X_num = np.load(self.cfg.preproc_dir / f"{split}_num.npy")
            y     = np.load(self.cfg.preproc_dir / f"{split}_y.npy")

            emb_vec = self._export_row_embeddings(X_cat)
            X_dense = np.hstack([emb_vec, X_num]).astype("float32")

            np.save(self.cfg.root_dir / f"X_{split}.npy", X_dense)
            np.save(self.cfg.root_dir / f"y_{split}.npy", y.astype("int64"))
            logger.info("Exported %s dense matrix shape %s", split, X_dense.shape)

        # schema
        schema = {
            "embedded_dim_total": int(emb_vec.shape[1]),
            "numeric_dim": int(self.numeric_dim),
            "feature_order": [f"emb_{i}" for i in range(emb_vec.shape[1])]
                            + [f"num_{i}" for i in range(self.numeric_dim)]
        }
        with open(self.cfg.root_dir / "embed_schema.json", "w") as f:
            json.dump(schema, f, indent=2)

        logger.info("Embedding export complete. Artifacts saved under %s", self.cfg.root_dir)

    def run(self):
        self.train()
        self.export()

===== END FILE: src/moneylion/components/embedding_export.py =====

===== BEGIN FILE: src/moneylion/components/model_trainer.py =====

from __future__ import annotations
import json
from pathlib import Path
from typing import Any, Dict, Tuple
from itertools import product

import numpy as np
import xgboost as xgb
from sklearn.metrics import (
    roc_auc_score, f1_score, precision_score, recall_score,
    accuracy_score, matthews_corrcoef, balanced_accuracy_score,
    confusion_matrix
)
import mlflow

from src.moneylion import logger
from src.moneylion.entity.config_entity import ModelTrainingConfig
from src.moneylion.utils.common import create_directories


class ModelTrainerBase:
    def __init__(self, cfg: ModelTrainingConfig) -> None:
        self.cfg = cfg
        create_directories([self.cfg.root_dir])

    def run(self) -> None:
        raise NotImplementedError


class XGBTrainer(ModelTrainerBase):
    STAGE_NAME = "Model Training"

    def _load_split(self, split: str) -> tuple[np.ndarray, np.ndarray]:
        X = np.load(self.cfg.embed_dir / f"X_{split}.npy")
        y = np.load(self.cfg.embed_dir / f"y_{split}.npy")
        return X.astype("float32"), y.astype("int64")

    @staticmethod
    def _choose_threshold_on_val(y_true: np.ndarray, y_prob: np.ndarray) -> Tuple[float, Dict[str, float]]:
        # Scan thresholds to maximize F1; return best threshold and metrics at that threshold
        best_t, best_f1 = 0.5, -1.0
        for t in np.linspace(0.05, 0.95, 91):
            y_hat = (y_prob >= t).astype(int)
            f1 = f1_score(y_true, y_hat, zero_division=0)
            if f1 > best_f1:
                best_f1 = f1
                best_t = float(t)
        y_hat = (y_prob >= best_t).astype(int)
        metrics = {
            "val_f1": f1_score(y_true, y_hat, zero_division=0),
            "val_precision": precision_score(y_true, y_hat, zero_division=0),
            "val_recall": recall_score(y_true, y_hat, zero_division=0),
            "val_accuracy": accuracy_score(y_true, y_hat),
            "val_mcc": matthews_corrcoef(y_true, y_hat) if len(np.unique(y_true)) == 2 else 0.0,
            "val_balanced_accuracy": balanced_accuracy_score(y_true, y_hat),
        }
        return best_t, metrics

    @staticmethod
    def _compute_test_metrics(y_true: np.ndarray, y_prob: np.ndarray, threshold: float) -> Tuple[Dict[str, float], np.ndarray]:
        y_hat = (y_prob >= threshold).astype(int)
        metrics = {
            "test_auc": roc_auc_score(y_true, y_prob),
            "test_f1": f1_score(y_true, y_hat, zero_division=0),
            "test_precision": precision_score(y_true, y_hat, zero_division=0),
            "test_recall": recall_score(y_true, y_hat, zero_division=0),
            "test_accuracy": accuracy_score(y_true, y_hat),
            "test_mcc": matthews_corrcoef(y_true, y_hat) if len(np.unique(y_true)) == 2 else 0.0,
            "test_balanced_accuracy": balanced_accuracy_score(y_true, y_hat),
            "decision_threshold": float(threshold),
        }
        cm = confusion_matrix(y_true, y_hat)
        return metrics, cm

    def run(self) -> None:
        # 0) MLflow experiment (optional)
        if self.cfg.mlflow_experiment:
            mlflow.set_experiment(self.cfg.mlflow_experiment)

        # 1) Load datasets
        X_train, y_train = self._load_split("train")
        X_val,   y_val   = self._load_split("val")
        X_test,  y_test  = self._load_split("test")

        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval   = xgb.DMatrix(X_val,   label=y_val)
        dtest  = xgb.DMatrix(X_test,  label=y_test)

        # 2) Grid search on validation AUC
        best_auc = -1.0
        best_params: Dict[str, Any] = {}
        best_booster: xgb.Booster | None = None

        param_grid = self.cfg.param_grid or {}
        keys, values = zip(*param_grid.items()) if param_grid else ([], [])
        for combo in product(*values) if values else [()]:
            params = dict(zip(keys, combo)) if combo else {}
            params.update(
                objective="binary:logistic",
                eval_metric=self.cfg.metric,
            )
            # Move n_estimators out of params to avoid XGBoost warning
            num_boost_round = int(params.pop("n_estimators", 200))

            with mlflow.start_run(nested=True):
                mlflow.log_params({**params, "num_boost_round": num_boost_round})

                booster = xgb.train(
                    params,
                    dtrain,
                    num_boost_round=num_boost_round,
                    evals=[(dval, "val")],
                    early_stopping_rounds=self.cfg.early_stopping_rounds,
                    verbose_eval=False,
                )

                val_pred = booster.predict(dval)
                val_auc = roc_auc_score(y_val, val_pred)
                mlflow.log_metric("val_auc", val_auc)
                logger.info("Params %s → val_auc=%.4f", {**params, "num_boost_round": num_boost_round}, val_auc)

                if val_auc > best_auc:
                    best_auc = val_auc
                    best_params = {**params, "num_boost_round": num_boost_round}
                    best_booster = booster

        assert best_booster is not None, "No model was trained."

        # 3) Choose decision threshold
        val_pred_best = best_booster.predict(dval)
        if self.cfg.decision_threshold.lower() == "auto":
            threshold, val_class_metrics = self._choose_threshold_on_val(y_val, val_pred_best)
        else:
            try:
                threshold = float(self.cfg.decision_threshold)
            except ValueError:
                threshold = 0.5
            val_hat = (val_pred_best >= threshold).astype(int)
            val_class_metrics = {
                "val_f1": f1_score(y_val, val_hat, zero_division=0),
                "val_precision": precision_score(y_val, val_hat, zero_division=0),
                "val_recall": recall_score(y_val, val_hat, zero_division=0),
                "val_accuracy": accuracy_score(y_val, val_hat),
                "val_mcc": matthews_corrcoef(y_val, val_hat) if len(np.unique(y_val)) == 2 else 0.0,
                "val_balanced_accuracy": balanced_accuracy_score(y_val, val_hat),
            }

        # 4) Final test metrics
        test_prob = best_booster.predict(dtest)
        test_metrics, cm = self._compute_test_metrics(y_test, test_prob, threshold)

        # 5) Persist model and reports
        model_path = self.cfg.root_dir / "xgb_model.json"
        best_booster.save_model(model_path)

        metrics_all = {
            "best_params": best_params,
            "val_auc": best_auc,
            **val_class_metrics,
            **test_metrics,
        }
        with open(self.cfg.root_dir / "metrics.json", "w") as f:
            json.dump(metrics_all, f, indent=2)

        # Save confusion matrix in JSON and CSV
        np.savetxt(self.cfg.root_dir / "confusion_matrix.csv", cm.astype(int), fmt="%d", delimiter=",")
        with open(self.cfg.root_dir / "confusion_matrix.json", "w") as f:
            json.dump({"labels": ["neg", "pos"], "matrix": cm.astype(int).tolist()}, f, indent=2)

        logger.info("Best params %s", best_params)
        logger.info(
            "Test AUC=%.4f  F1=%.4f  Precision=%.4f  Recall=%.4f  Acc=%.4f  MCC=%.4f  Thr=%.3f",
            test_metrics["test_auc"], test_metrics["test_f1"], test_metrics["test_precision"],
            test_metrics["test_recall"], test_metrics["test_accuracy"], test_metrics["test_mcc"],
            test_metrics["decision_threshold"],
        )
        logger.info("Model saved to %s", model_path)

        # 6) MLflow logging for the winning run
        with mlflow.start_run(nested=True):
            mlflow.log_params(best_params)
            for k, v in metrics_all.items():
                if isinstance(v, (int, float)):
                    mlflow.log_metric(k, v)
            mlflow.log_artifact(model_path, artifact_path="model")
            mlflow.log_artifact(self.cfg.root_dir / "metrics.json", artifact_path="reports")
            mlflow.log_artifact(self.cfg.root_dir / "confusion_matrix.csv", artifact_path="reports")
            mlflow.log_artifact(self.cfg.root_dir / "confusion_matrix.json", artifact_path="reports")

===== END FILE: src/moneylion/components/model_trainer.py =====

===== BEGIN FILE: src/moneylion/config/configuration.py =====
from src.moneylion.constants import *
from src.moneylion.utils.common import read_yaml, create_directories
from src.moneylion.entity.config_entity import (
    DataIngestionConfig, DataTransformationConfig, DataPreprocessingConfig, DataEmbeddingConfig, ModelTrainingConfig
)
from pathlib import Path

class ConfigurationManager:
    def __init__(self, 
                 config_filepath=CONFIG_FILE_PATH,
                 params_filepath=PARAMS_FILE_PATH
                 ):
        self.config = read_yaml(config_filepath)
        self.params = read_yaml(params_filepath)

        create_directories([self.config.artifacts_root])

    def get_data_ingestion_config(self) -> DataIngestionConfig:
        config=self.config.data_ingestion
        create_directories([config.root_dir])

        data_ingestion_config = DataIngestionConfig(
            root_dir=config.root_dir,
            gcs_bucket_name=config.gcs_bucket_name,
            gcs_source_folder = config.gcs_source_folder,
            raw_files=config.raw_files,
            local_download_dir=config.local_download_dir,
            # absolute path
            gcp_credentials_path = Path(config.gcp_credentials_path).resolve(), 
        )
        return data_ingestion_config
    
    def get_data_transformation_config(self) -> DataTransformationConfig:
        config = self.config.data_transformation
        create_directories([config.root_dir])
        return DataTransformationConfig(
            root_dir        = config.root_dir,
            loan_raw        = config.loan_raw,
            clarity_raw     = config.clarity_raw,
            joined_local    = config.joined_local
        )

    def get_data_preprocessing_config(self) -> DataPreprocessingConfig:
        cfg = self.config.data_preprocessing
        create_directories([cfg.root_dir])
        return DataPreprocessingConfig(
            root_dir=Path(cfg.root_dir),
            joined_csv=Path(cfg.joined_csv),
            test_size=float(cfg.test_size),
            val_size=float(cfg.val_size),
            random_state=int(cfg.random_state),
        )

    def get_data_embedding_config(self) -> DataEmbeddingConfig:
        cfg = self.config.data_embedding
        create_directories([cfg.root_dir])
        return DataEmbeddingConfig(
            root_dir          = Path(cfg.root_dir),
            preproc_dir       = Path(cfg.preproc_dir),
            epochs            = int(cfg.epochs),
            batch_size        = int(cfg.batch_size),
            lr                = float(cfg.lr),
            random_state      = int(cfg.random_state),
            embedding_dim_rule= str(cfg.embedding_dim_rule),
        )
    
    def get_model_training_config(self) -> ModelTrainingConfig:
        cfg = self.config.model_training
        create_directories([cfg.root_dir])
        return ModelTrainingConfig(
            root_dir               = Path(cfg.root_dir),
            embed_dir              = Path(cfg.embed_dir),
            model_type             = cfg.model_type,
            param_grid             = cfg.param_grid,
            early_stopping_rounds  = int(cfg.early_stopping_rounds),
            metric                 = cfg.metric,
            decision_threshold     = str(cfg.decision_threshold),
            mlflow_experiment      = str(cfg.mlflow_experiment),
        )


===== END FILE: src/moneylion/config/configuration.py =====

===== BEGIN FILE: src/moneylion/constants/__init__.py =====
from pathlib import Path

CONFIG_FILE_PATH=Path('yamls/config.yaml')
PARAMS_FILE_PATH=Path('yamls/params.yaml')

===== END FILE: src/moneylion/constants/__init__.py =====

===== BEGIN FILE: src/moneylion/entity/config_entity.py =====
from dataclasses import dataclass
from pathlib import Path

@dataclass
class DataIngestionConfig:
    root_dir: Path
    gcs_bucket_name: str
    gcs_source_folder: str
    raw_files: list[str]
    local_download_dir: Path
    gcp_credentials_path: Path

@dataclass
class DataTransformationConfig:
    root_dir: Path
    loan_raw: Path
    clarity_raw: Path
    joined_local: Path

@dataclass
class DataLoadingConfig:
    root_dir: Path
    local_file: Path
    gcs_target: str

@dataclass
class DataPreprocessingConfig:
    root_dir: Path
    joined_csv: Path
    test_size: float
    val_size: float
    random_state: int 

@dataclass
class DataEmbeddingConfig:
    root_dir: Path
    preproc_dir: Path
    epochs: int
    batch_size: int
    lr: float
    random_state: int
    embedding_dim_rule: str  

@dataclass
class ModelTrainingConfig:
    root_dir: Path
    embed_dir: Path
    model_type: str
    param_grid: dict
    early_stopping_rounds: int
    metric: str
    decision_threshold: str         
    mlflow_experiment: str | None = None

===== END FILE: src/moneylion/entity/config_entity.py =====

===== BEGIN FILE: src/moneylion/pipeline/data_ingestion_pipeline.py =====
import os
from pathlib import Path
from dotenv import load_dotenv
from src.moneylion import logger
from src.moneylion.components.data_ingestion import DataIngestion
from src.moneylion.config.configuration import ConfigurationManager


class DataIngestionPipeline:

    def __init__(self) -> None:
        self.config_manager = ConfigurationManager()
        self.STAGE_NAME = "Data Ingestion"

    def initiate_data_ingestion(self) -> bool:

        # Get the configuration for the data ingestion stage
        ingestion_config = self.config_manager.get_data_ingestion_config()
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(ingestion_config.gcp_credentials_path)

        # Instantiate and run the component
        ingestion_component = DataIngestion(config=ingestion_config)
        ingestion_component.download_files()

        return True

if __name__ == '__main__':
    try :
        obj = DataIngestionPipeline()
        logger.info(f">>>> Stage '{obj.STAGE_NAME}' started <<<<")
        obj.initiate_data_ingestion()
        logger.info(f">>>> Stage '{obj.STAGE_NAME}' completed successfully <<<<\n")
    except Exception as e:
        logger.exception(e)
===== END FILE: src/moneylion/pipeline/data_ingestion_pipeline.py =====

===== BEGIN FILE: src/moneylion/pipeline/data_preprocessing_pipeline.py =====
# pipeline/data_preprocessing_pipeline.py
from src.moneylion import logger
from src.moneylion.components.data_preprocessing import DataPreprocessor
from src.moneylion.config.configuration import ConfigurationManager

class DataPreprocessingPipeline:
    STAGE_NAME = "Data Preprocessing"

    def __init__(self) -> None:
        cfg_mgr = ConfigurationManager()
        self.cfg  = cfg_mgr.get_data_preprocessing_config()

    def initiate_data_preprocessing(self) -> bool:
        DataPreprocessor(self.cfg).run()
        return True

===== END FILE: src/moneylion/pipeline/data_preprocessing_pipeline.py =====

===== BEGIN FILE: src/moneylion/pipeline/data_transformation_pipeline.py =====
import os
from pathlib import Path
from src.moneylion import logger
from src.moneylion.components.data_transformation import DataTransformation
from src.moneylion.config.configuration import ConfigurationManager


class DataTransformationPipeline:

    def __init__(self) -> None:
        self.config_manager = ConfigurationManager()
        self.STAGE_NAME = "Data Transformation"

    def initiate_data_transformation(self) -> bool:

        # Get the configuration for the data ingestion stage
        config = self.config_manager.get_data_transformation_config()

        # Instantiate and run the component
        transformation_component = DataTransformation(config=config)
        transformation_component.transform_data()

        return True

if __name__ == '__main__':
    try :
        obj = DataTransformationPipeline()
        logger.info(f">>>> Stage '{obj.STAGE_NAME}' started <<<<")
        obj.initiate_data_ingestion()
        logger.info(f">>>> Stage '{obj.STAGE_NAME}' completed successfully <<<<\n")
    except Exception as e:
        logger.exception(e)
===== END FILE: src/moneylion/pipeline/data_transformation_pipeline.py =====

===== BEGIN FILE: src/moneylion/pipeline/embedding_pipeline.py =====
from src.moneylion import logger
from src.moneylion.components.embedding_export import EmbeddingExporter
from src.moneylion.config.configuration import ConfigurationManager

class EmbeddingPipeline:
    STAGE_NAME = "Data Embedding & Export"

    def __init__(self):
        self.cfg = ConfigurationManager().get_data_embedding_config()

    def initiate_embedding(self) -> bool:
        EmbeddingExporter(self.cfg).run()
        return True

===== END FILE: src/moneylion/pipeline/embedding_pipeline.py =====

===== BEGIN FILE: src/moneylion/pipeline/model_training_pipeline.py =====
from src.moneylion import logger
from src.moneylion.components.model_trainer import XGBTrainer
from src.moneylion.config.configuration import ConfigurationManager

class ModelTrainingPipeline:
    STAGE_NAME = "Model Training"

    def __init__(self):
        cfg_mgr = ConfigurationManager()
        self.cfg = cfg_mgr.get_model_training_config()

    def initiate_model_training(self) -> bool:
        XGBTrainer(self.cfg).run()
        return True

===== END FILE: src/moneylion/pipeline/model_training_pipeline.py =====

===== BEGIN FILE: src/moneylion/utils/common.py =====
import os
import yaml
from src.moneylion import logger
import json
import joblib
from ensure import ensure_annotations
from box import ConfigBox
from pathlib import Path
from typing import Any 
from box.exceptions import BoxValueError

@ensure_annotations
def read_yaml( path_to_yaml: Path) -> ConfigBox:
    """reads yaml file and returns
    
    Args:
        path_to_yaml (str) : path like input 

    Raises:
        ValueError: if yaml file is empty
        e: empth file

    Returns:
        ConfigBox: ConfigBox type
    """
    try:
        with open( path_to_yaml) as yaml_file:
            content = yaml.safe_load(yaml_file)
            logger.info(f'yaml file: {path_to_yaml} loaded successfully')
            return ConfigBox(content)
    except BoxValueError:
        raise ValueError('yaml file is empty')
    except Exception as e:
        raise e
    

@ensure_annotations
def create_directories( path_to_directories: list, verbose=True):
    """create list of directories

    Args:
        path_to_directories (list)
        ignore_log (bool, optional)

    """
    for path in path_to_directories:
        os.makedirs(path, exist_ok=True)
        if verbose: 
            logger.info(f'created directory at: {path}')   


@ensure_annotations
def save_json(path: Path, data: dict):
    """save json data
    Args:
        path (Path)
        data (dict)
    """
    with open(path, 'w') as f:
        json.dump(data, f, indent=4)

    logger.info(f'json file saved at: {path}')


@ensure_annotations
def load_json(path: Path) -> ConfigBox:
    """loads json files data
    Args:
        path (Path)
    Returns:
        data as class attributes instead of dict (ConfigBox)
    """
    with open(path) as f:
        content=json.load(f)

    logger.info(f'json file loaded successfully from: {path}')
    return ConfigBox(content)


@ensure_annotations
def save_bin(data: Any, path: Path):
    """save binary file
    Args:
        data (Any)
        path (Path)
    """
    joblib.dump(value=data, filename=path)
    logger.info(f'binary file saved at {path}')


@ensure_annotations
def load_bin(path: Path) -> Any:
    """load binary data
    """
    data = joblib.load(path)
    logger.info(f'binary file loaded from: {path}')
    return data
===== END FILE: src/moneylion/utils/common.py =====

===== BEGIN FILE: yamls/config.yaml =====
artifacts_root: artifacts

data_ingestion:
  root_dir: artifacts/data_ingestion
  gcs_bucket_name: moneylion-202511010
  gcs_source_folder: raw 
  raw_files:
    - clarity_underwriting_variables.csv
    - loan.csv
    - payment.csv
  local_download_dir: artifacts/data_ingestion/raw
  gcp_credentials_path: .credentials/gcp/daring-night-475804-a8-ab772b8e37f7.json

data_transformation:
  root_dir: artifacts/data_transformation
  loan_raw: artifacts/data_ingestion/raw/loan.csv
  clarity_raw: artifacts/data_ingestion/raw/clarity_underwriting_variables.csv
  joined_local: artifacts/data_transformation/joined_df.csv

data_loading:
  root_dir: artifacts/data_loading
  local_file: artifacts/data_transformation/joined_df.csv
  gcs_target: gs://moneylion-202511010/joined_df.csv

data_preprocessing:
  root_dir: artifacts/data_preprocessing
  joined_csv: artifacts/data_transformation/joined_df.csv
  test_size: 0.15
  val_size: 0.15
  random_state: 517

data_embedding:
  root_dir: artifacts/data_embedding
  preproc_dir: artifacts/data_preprocessing
  epochs: 3
  batch_size: 512
  lr: 1e-3
  random_state: 517
  embedding_dim_rule: sqrt

model_training:
  root_dir: artifacts/model_training
  embed_dir: artifacts/data_embedding
  model_type: xgboost
  param_grid:
    max_depth:  [3, 5]
    eta:        [0.3, 0.1]
    n_estimators: [200]
    subsample:  [0.8]
    colsample_bytree: [0.8]
  early_stopping_rounds: 20
  metric: auc
  decision_threshold: auto         
  mlflow_experiment: loan-default-xgb-v1


===== END FILE: yamls/config.yaml =====

===== BEGIN FILE: yamls/params.yaml =====
'key': 'value'
===== END FILE: yamls/params.yaml =====
