#!/usr/bin/env python3
"""
A command-line utility to collate project source files into a single text file.

This script reads a JSON file generated by the `tree -J` command, which describes
a project's directory structure. It then walks this structure, reads the content
of each file, and appends it to a single output file, prefixed with a header
containing the file's relative path.

It includes options to filter files by glob patterns, skip large or binary files,
and specify the project's base directory on the filesystem.

Usage:
    python collector.py [OPTIONS] <json_input> <output_file>

Example:
    python collector.py moneyLion-2025-11010.json project_source_code.txt
"""

from __future__ import annotations

import argparse
import base64
import json
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional


def parse_args() -> argparse.Namespace:
    """
    Parses command-line arguments for the script.

    Returns:
        An argparse.Namespace object containing the parsed arguments.
    """
    parser = argparse.ArgumentParser(
        description="Collate project files listed in a `tree -J` JSON file into a single text file."
    )
    parser.add_argument(
        "json_input",
        help="Path to the input `tree -J` JSON file (e.g., 'moneyLion-2025-11010.json').",
    )
    parser.add_argument(
        "output_file", help="Path for the final, collated output text file."
    )
    parser.add_argument(
        "-b",
        "--base-dir",
        default=".",
        help="The filesystem base directory that corresponds to the JSON's root (default: current directory).",
    )
    parser.add_argument(
        "--include",
        nargs="*",
        default=None,
        help="Glob patterns for files to include (e.g., '*.py' '*.yaml'). If not provided, all files are considered.",
    )
    parser.add_argument(
        "--exclude",
        nargs="*",
        default=[
            "*.pyc",
            "__pycache__/*",
            "artifacts/*",
            ".git/*",
            "*.ipynb",
            "*.csv",
            "logs/*",
            ".credentials/*",
            ".env",
        ],
        help="Glob patterns for files or directories to exclude. Default patterns exclude common non-source files.",
    )
    parser.add_argument(
        "--max-bytes",
        type=int,
        default=2_000_000,
        help="Maximum file size in bytes to include. Files larger than this will be skipped (default: 2MB).",
    )
    parser.add_argument(
        "--binary-mode",
        choices=["skip", "base64"],
        default="skip",
        help="How to handle binary files or files with decoding errors: 'skip' (default) or 'base64' encode.",
    )
    return parser.parse_args()


def _is_match(path: Path, patterns: Optional[List[str]]) -> bool:
    """Checks if a path matches any of the provided glob patterns."""
    if not patterns:
        return False
    return any(path.match(pattern) for pattern in patterns)


def _walk_tree_json(
    node: Dict[str, Any], parent_path: Path
) -> Iterable[Path]:
    """
    Recursively walks the nested JSON structure and yields the path for each file.

    The expected JSON structure is a list of dictionaries, where each dictionary
    has 'type', 'name', and optionally 'contents' keys.

    Args:
        node: The current dictionary node in the JSON tree.
        parent_path: The filesystem path of the parent directory.

    Yields:
        A Path object for each file found in the tree.
    """
    node_type = node.get("type")
    name = node.get("name", "")

    # The root node often has name '.', which we can ignore
    current_path = parent_path / name if name != "." else parent_path

    if node_type == "file":
        yield current_path
    elif node_type == "directory":
        for child in node.get("contents", []):
            yield from _walk_tree_json(child, current_path)


def load_paths_from_json(json_path: Path) -> List[Path]:
    """
    Loads a `tree -J` JSON file and extracts all file paths.

    Args:
        json_path: Path to the JSON file.

    Returns:
        A list of relative Path objects for every file in the JSON structure.
    """
    with open(json_path, "r", encoding="utf-8") as f:
        tree_data = json.load(f)

    file_paths: List[Path] = []
    # The top-level structure is a list; the first element is the directory tree.
    for element in tree_data:
        if isinstance(element, dict) and element.get("type") == "directory":
            file_paths.extend(list(_walk_tree_json(element, Path())))
            break  # Assume only one main directory tree per file

    return file_paths


def collate_source_code(
    json_input_path: Path,
    output_file_path: Path,
    base_dir: Path,
    include_patterns: Optional[List[str]],
    exclude_patterns: Optional[List[str]],
    max_bytes: int,
    binary_mode: str,
) -> None:
    """
    Main function to perform the collation of source files.

    Args:
        json_input_path: Path to the `tree -J` JSON file.
        output_file_path: Path where the final collated file will be saved.
        base_dir: The root directory on disk where the source files are located.
        include_patterns: List of glob patterns for files to include.
        exclude_patterns: List of glob patterns for files/dirs to exclude.
        max_bytes: Maximum file size to process.
        binary_mode: How to handle binary files ('skip' or 'base64').
    """
    relative_paths = load_paths_from_json(json_input_path)
    absolute_paths = sorted([(base_dir / p).resolve() for p in relative_paths])

    files_written_count = 0
    with open(output_file_path, "w", encoding="utf-8") as out_file:
        for abs_path in absolute_paths:
            if not abs_path.is_file():
                continue

            rel_path_for_header = abs_path.relative_to(base_dir.resolve())

            # Apply include/exclude filters
            if include_patterns and not _is_match(rel_path_for_header, include_patterns):
                continue
            if exclude_patterns and _is_match(rel_path_for_header, exclude_patterns):
                continue

            # Apply size filter
            if abs_path.stat().st_size > max_bytes:
                out_file.write(
                    f"\n===== SKIPPED (size > {max_bytes} bytes): {rel_path_for_header} =====\n"
                )
                continue

            # Read file content, handling potential binary files
            try:
                content = abs_path.read_text(encoding="utf-8")
            except UnicodeDecodeError:
                if binary_mode == "skip":
                    out_file.write(
                        f"\n===== SKIPPED (binary or non-utf8): {rel_path_for_header} =====\n"
                    )
                    continue
                elif binary_mode == "base64":
                    content = base64.b64encode(abs_path.read_bytes()).decode("ascii")
                    out_file.write(
                        f"\n===== BEGIN BINARY (BASE64): {rel_path_for_header} =====\n"
                    )
                    out_file.write(content)
                    out_file.write(
                        f"\n===== END BINARY (BASE64): {rel_path_for_header} =====\n"
                    )
                    files_written_count += 1
                    continue
            
            # Write header and content for standard text files
            out_file.write(f"\n===== BEGIN FILE: {rel_path_for_header} =====\n")
            out_file.write(content)
            out_file.write(f"\n===== END FILE: {rel_path_for_header} =====\n")
            files_written_count += 1

    print(f"Successfully wrote {files_written_count} file(s) to '{output_file_path}'")


def main():
    """Script entry point."""
    args = parse_args()
    collate_source_code(
        json_input_path=Path(args.json_input),
        output_file_path=Path(args.output_file),
        base_dir=Path(args.base_dir),
        include_patterns=args.include,
        exclude_patterns=args.exclude,
        max_bytes=args.max_bytes,
        binary_mode=args.binary_mode,
    )


if __name__ == "__main__":
    main()

