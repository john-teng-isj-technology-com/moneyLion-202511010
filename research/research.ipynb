{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5956812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 114,588 rows\n",
      "Summary:\n",
      "predicted_class         0         1\n",
      "split                              \n",
      "test             0.098261  0.901739\n",
      "train            0.097867  0.902133\n",
      "val              0.099139  0.900861\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_yaml(path: Path) -> dict:\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def rebuild_splits(joined_csv: Path, test_size: float, val_size: float, random_state: int):\n",
    "\n",
    "    df = pd.read_csv(joined_csv, low_memory=False)\n",
    "    if \"loanId\" not in df.columns or \"isBadDebt\" not in df.columns:\n",
    "        raise ValueError(\"joined_df.csv must contain loanId and isBadDebt columns.\")\n",
    "    y = df[\"isBadDebt\"].to_numpy(np.int64)\n",
    "    n = len(y)\n",
    "    all_idx = np.arange(n)\n",
    "\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        all_idx,\n",
    "        test_size=test_size + val_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    rel_test = test_size / (test_size + val_size)\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx,\n",
    "        test_size=rel_test,\n",
    "        stratify=y[temp_idx],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return df[\"loanId\"].astype(str).tolist(), y, train_idx, val_idx, test_idx\n",
    "\n",
    "\n",
    "def main():\n",
    "    repo_root = Path(\".\").resolve()\n",
    "\n",
    "    cfg = load_yaml(repo_root / \"yamls\" / \"config.yaml\")\n",
    "    prm = load_yaml(repo_root / \"yamls\" / \"params.yaml\")\n",
    "\n",
    "    joined_csv = Path(cfg[\"data_transformation\"][\"joined_local\"])\n",
    "    embed_dir = Path(cfg[\"data_embedding\"][\"root_dir\"])\n",
    "    model_dir = Path(cfg[\"model_training\"][\"root_dir\"])\n",
    "    raw_loan_csv = Path(cfg[\"data_ingestion\"][\"local_download_dir\"]) / \"loan.csv\"\n",
    "\n",
    "    test_size = float(prm[\"data_preprocessing\"][\"test_size\"])\n",
    "    val_size = float(prm[\"data_preprocessing\"][\"val_size\"])\n",
    "    random_state = int(prm[\"data_preprocessing\"][\"random_state\"])\n",
    "\n",
    "    loan_ids_all, y_all, train_idx, val_idx, test_idx = rebuild_splits(\n",
    "        joined_csv, test_size, val_size, random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.load(embed_dir / \"X_train.npy\")\n",
    "    y_train = np.load(embed_dir / \"y_train.npy\").astype(np.int64)\n",
    "\n",
    "    X_val = np.load(embed_dir / \"X_val.npy\")\n",
    "    y_val = np.load(embed_dir / \"y_val.npy\").astype(np.int64)\n",
    "\n",
    "    X_test = np.load(embed_dir / \"X_test.npy\")\n",
    "    y_test = np.load(embed_dir / \"y_test.npy\").astype(np.int64)\n",
    "\n",
    "    assert len(train_idx) == len(X_train) == len(y_train), \"Train split length mismatch.\"\n",
    "    assert len(val_idx) == len(X_val) == len(y_val), \"Val split length mismatch.\"\n",
    "    assert len(test_idx) == len(X_test) == len(y_test), \"Test split length mismatch.\"\n",
    "\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(model_dir / \"xgb_model.json\")\n",
    "\n",
    "    with open(model_dir / \"metrics.json\", \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    threshold = float(metrics.get(\"decision_threshold\", 0.5))\n",
    "\n",
    "    def predict_block(X: np.ndarray) -> np.ndarray:\n",
    "        dmat = xgb.DMatrix(X)\n",
    "        return booster.predict(dmat)\n",
    "\n",
    "    p_train = predict_block(X_train)\n",
    "    p_val = predict_block(X_val)\n",
    "    p_test = predict_block(X_test)\n",
    "\n",
    "    def build_df(split_name: str, idx: np.ndarray, probs: np.ndarray, y_slice: np.ndarray) -> pd.DataFrame:\n",
    "        loan_ids = [loan_ids_all[i] for i in idx.tolist()]\n",
    "        y_true = y_slice.astype(int)\n",
    "        y_pred_cls = (probs >= threshold).astype(int)\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"loanId\": loan_ids,\n",
    "                \"split\": split_name,\n",
    "                \"isBadDebt\": y_true,\n",
    "                \"predicted_prob\": probs,\n",
    "                \"predicted_class\": y_pred_cls,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_train = build_df(\"train\", train_idx, p_train, y_train)\n",
    "    df_val = build_df(\"val\", val_idx, p_val, y_val)\n",
    "    df_test = build_df(\"test\", test_idx, p_test, y_test)\n",
    "    preds_df = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "\n",
    "    raw_cols = [\"loanId\", \"loanAmount\", \"originallyScheduledPaymentAmount\", \"isFunded\"]\n",
    "    loan_raw = pd.read_csv(raw_loan_csv, low_memory=False, usecols=lambda c: c in raw_cols)\n",
    "    loan_raw[\"loanId\"] = loan_raw[\"loanId\"].astype(str)\n",
    "    if \"isFunded\" in loan_raw.columns:\n",
    "        loan_raw[\"isFunded\"] = loan_raw[\"isFunded\"].astype(int)\n",
    "    loan_raw = loan_raw.drop_duplicates(subset=[\"loanId\"], keep=\"last\")\n",
    "\n",
    "    preds_df = preds_df.merge(loan_raw, on=\"loanId\", how=\"left\")\n",
    "\n",
    "    cols = [\n",
    "        \"loanId\",\n",
    "        \"split\",\n",
    "        \"isBadDebt\",\n",
    "        \"predicted_prob\",\n",
    "        \"predicted_class\",\n",
    "        \"loanAmount\",\n",
    "        \"originallyScheduledPaymentAmount\",\n",
    "        \"isFunded\",\n",
    "    ]\n",
    "    preds_df = preds_df[cols]\n",
    "\n",
    "    out_path = repo_root / 'artifacts' / 'analysis' / \"predictions.csv\"\n",
    "    preds_df.to_csv(out_path, index=False)\n",
    "    print(f\"Wrote {len(preds_df):,} rows\")\n",
    "\n",
    "    print(\n",
    "        \"Summary:\",\n",
    "        preds_df.groupby(\"split\")[\"predicted_class\"].value_counts(normalize=True).unstack(fill_value=0.0),\n",
    "        sep=\"\\n\",\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a64c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Impact Summary ===\n",
      "Funded value baseline: 30.11% of total\n",
      "Funded value after model: 25.86% (-4.25 pp)\n",
      "Bad-debt share of funded (baseline): 58.31%\n",
      "Bad-debt share of funded (after): 12.68% (-45.62 pp)\n",
      "Bad funded correctly flagged: 78.25% of funded bad value\n",
      "Good funded incorrectly flagged: 14.11% of funded value\n",
      "Not funded but predicted good: 0.00% of total value\n",
      "Profit margin baseline: 10.19%\n",
      "Profit margin after model: 69.93% (uplift +59.74 pp)\n",
      "Wrote /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/impact_metrics.json\n",
      "Wrote /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/impact_table.csv\n",
      "Copied /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/metrics.json and /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/confusion_matrix.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _pct(numer, denom):\n",
    "    numer = float(numer)\n",
    "    denom = float(denom)\n",
    "    return (numer / denom * 100.0) if denom > 0 else 0.0\n",
    "\n",
    "def main():\n",
    "    repo = Path(\".\").resolve()\n",
    "    analysis_dir = repo / \"artifacts\" / \"analysis\"\n",
    "    model_dir = repo / \"artifacts\" / \"model_training\"\n",
    "    analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    preds_path = analysis_dir / \"predictions.csv\"\n",
    "    metrics_path = model_dir / \"metrics.json\"\n",
    "    cm_path = model_dir / \"confusion_matrix.json\"\n",
    "\n",
    "    if not preds_path.exists():\n",
    "        raise FileNotFoundError(str(preds_path))\n",
    "    if not metrics_path.exists():\n",
    "        raise FileNotFoundError(str(metrics_path))\n",
    "    if not cm_path.exists():\n",
    "        raise FileNotFoundError(str(cm_path))\n",
    "\n",
    "    df = pd.read_csv(preds_path)\n",
    "\n",
    "    for col in [\"isBadDebt\", \"predicted_class\", \"isFunded\"]:\n",
    "        df[col] = pd.to_numeric(df.get(col, 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "    for col in [\"loanAmount\", \"originallyScheduledPaymentAmount\", \"predicted_prob\"]:\n",
    "        df[col] = pd.to_numeric(df.get(col, 0.0), errors=\"coerce\").fillna(0.0).astype(float)\n",
    "\n",
    "    total_principal_all_loans = float(df[\"loanAmount\"].sum())\n",
    "    funded_df = df[df[\"isFunded\"] == 1].copy()\n",
    "    total_principal_funded_baseline = float(funded_df[\"loanAmount\"].sum())\n",
    "    pct_funded_value_baseline = _pct(total_principal_funded_baseline, total_principal_all_loans)\n",
    "\n",
    "    funded_bad_df = funded_df[funded_df[\"isBadDebt\"] == 1]\n",
    "    funded_good_df = funded_df[funded_df[\"isBadDebt\"] == 0]\n",
    "\n",
    "    total_principal_funded_bad_baseline = float(funded_bad_df[\"loanAmount\"].sum())\n",
    "    total_principal_funded_good_baseline = float(funded_good_df[\"loanAmount\"].sum())\n",
    "\n",
    "    pct_bad_debt_share_of_funded_value_baseline = _pct(\n",
    "        total_principal_funded_bad_baseline, total_principal_funded_baseline\n",
    "    )\n",
    "    pct_good_debt_share_of_funded_value_baseline = _pct(\n",
    "        total_principal_funded_good_baseline, total_principal_funded_baseline\n",
    "    )\n",
    "\n",
    "    funded_bad_tp_df = funded_bad_df[funded_bad_df[\"predicted_class\"] == 1]\n",
    "    total_principal_funded_bad_correctly_flagged = float(funded_bad_tp_df[\"loanAmount\"].sum())\n",
    "    pct_of_funded_bad_value_correctly_flagged = _pct(\n",
    "        total_principal_funded_bad_correctly_flagged, total_principal_funded_bad_baseline\n",
    "    )\n",
    "\n",
    "    total_principal_funded_bad_after_model = (\n",
    "        total_principal_funded_bad_baseline - total_principal_funded_bad_correctly_flagged\n",
    "    )\n",
    "    pct_bad_debt_share_of_funded_value_after_model = _pct(\n",
    "        total_principal_funded_bad_after_model, total_principal_funded_baseline\n",
    "    )\n",
    "    pct_point_change_bad_debt_share_of_funded_value = (\n",
    "        pct_bad_debt_share_of_funded_value_after_model - pct_bad_debt_share_of_funded_value_baseline\n",
    "    )\n",
    "\n",
    "    funded_good_fp_df = funded_good_df[funded_good_df[\"predicted_class\"] == 1]\n",
    "    total_principal_funded_good_incorrectly_flagged = float(funded_good_fp_df[\"loanAmount\"].sum())\n",
    "    pct_of_funded_good_value_incorrectly_flagged = _pct(\n",
    "        total_principal_funded_good_incorrectly_flagged, total_principal_funded_baseline\n",
    "    )\n",
    "\n",
    "    not_funded_df = df[df[\"isFunded\"] == 0]\n",
    "    not_funded_pred_good_df = not_funded_df[not_funded_df[\"predicted_class\"] == 0]\n",
    "    total_principal_not_funded_predicted_good = float(\n",
    "        not_funded_pred_good_df[\"loanAmount\"].sum()\n",
    "    )\n",
    "    pct_of_total_value_not_funded_but_predicted_good = _pct(\n",
    "        total_principal_not_funded_predicted_good, total_principal_all_loans\n",
    "    )\n",
    "\n",
    "    total_principal_funded_after_model = (\n",
    "        total_principal_funded_baseline\n",
    "        - total_principal_funded_good_incorrectly_flagged\n",
    "        + total_principal_not_funded_predicted_good\n",
    "    )\n",
    "    pct_funded_value_after_model = _pct(\n",
    "        total_principal_funded_after_model, total_principal_all_loans\n",
    "    )\n",
    "    pct_point_change_funded_value = (\n",
    "        pct_funded_value_after_model - pct_funded_value_baseline\n",
    "    )\n",
    "\n",
    "    total_revenue_from_funded_good_baseline = float(\n",
    "        funded_good_df[\"originallyScheduledPaymentAmount\"].sum()\n",
    "    )\n",
    "    total_profit_baseline_amount = (\n",
    "        total_revenue_from_funded_good_baseline - total_principal_funded_baseline\n",
    "    )\n",
    "    pct_profit_margin_baseline = _pct(\n",
    "        total_profit_baseline_amount, total_principal_funded_baseline\n",
    "    )\n",
    "\n",
    "    total_principal_retained_after_filter = float(\n",
    "        funded_df[funded_df[\"predicted_class\"] == 0][\"loanAmount\"].sum()\n",
    "    )\n",
    "    total_profit_after_model_amount = (\n",
    "        total_revenue_from_funded_good_baseline - total_principal_retained_after_filter\n",
    "    )\n",
    "    pct_profit_margin_after_model = _pct(\n",
    "        total_profit_after_model_amount, total_principal_funded_baseline\n",
    "    )\n",
    "    pct_point_uplift_in_profit_margin = (\n",
    "        pct_profit_margin_after_model - pct_profit_margin_baseline\n",
    "    )\n",
    "\n",
    "    with open(metrics_path, \"r\") as f:\n",
    "        model_metrics = json.load(f)\n",
    "    with open(cm_path, \"r\") as f:\n",
    "        confusion = json.load(f)\n",
    "\n",
    "    impact = {\n",
    "        \"pct_funded_value_baseline\": pct_funded_value_baseline,\n",
    "        \"pct_funded_value_after_model\": pct_funded_value_after_model,\n",
    "        \"pct_point_change_funded_value\": pct_point_change_funded_value,\n",
    "        \"pct_bad_debt_share_of_funded_value_baseline\": pct_bad_debt_share_of_funded_value_baseline,\n",
    "        \"pct_bad_debt_share_of_funded_value_after_model\": pct_bad_debt_share_of_funded_value_after_model,\n",
    "        \"pct_point_change_bad_debt_share_of_funded_value\": pct_point_change_bad_debt_share_of_funded_value,\n",
    "        \"pct_good_debt_share_of_funded_value_baseline\": pct_good_debt_share_of_funded_value_baseline,\n",
    "        \"pct_of_funded_bad_value_correctly_flagged\": pct_of_funded_bad_value_correctly_flagged,\n",
    "        \"pct_of_funded_good_value_incorrectly_flagged\": pct_of_funded_good_value_incorrectly_flagged,\n",
    "        \"pct_of_total_value_not_funded_but_predicted_good\": pct_of_total_value_not_funded_but_predicted_good,\n",
    "        \"pct_profit_margin_baseline\": pct_profit_margin_baseline,\n",
    "        \"pct_profit_margin_after_model\": pct_profit_margin_after_model,\n",
    "        \"pct_point_uplift_in_profit_margin\": pct_point_uplift_in_profit_margin,\n",
    "        \"total_principal_all_loans\": total_principal_all_loans,\n",
    "        \"total_principal_funded_baseline\": total_principal_funded_baseline,\n",
    "        \"total_principal_funded_after_model\": total_principal_funded_after_model,\n",
    "        \"total_principal_funded_bad_baseline\": total_principal_funded_bad_baseline,\n",
    "        \"total_principal_funded_bad_correctly_flagged\": total_principal_funded_bad_correctly_flagged,\n",
    "        \"total_principal_funded_good_incorrectly_flagged\": total_principal_funded_good_incorrectly_flagged,\n",
    "        \"total_principal_not_funded_predicted_good\": total_principal_not_funded_predicted_good,\n",
    "        \"total_revenue_from_funded_good_baseline\": total_revenue_from_funded_good_baseline,\n",
    "        \"total_profit_baseline_amount\": total_profit_baseline_amount,\n",
    "        \"total_profit_after_model_amount\": total_profit_after_model_amount,\n",
    "    }\n",
    "\n",
    "    out_json = analysis_dir / \"impact_metrics.json\"\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"impact\": impact,\n",
    "                \"model_metrics\": model_metrics,\n",
    "                \"confusion_matrix\": confusion,\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    rows = [\n",
    "        (\"pct_funded_value_baseline\", impact[\"pct_funded_value_baseline\"], \"Percent of total loan value funded (baseline)\"),\n",
    "        (\"pct_funded_value_after_model\", impact[\"pct_funded_value_after_model\"], \"Percent of total loan value funded (after model)\"),\n",
    "        (\"pct_point_change_funded_value\", impact[\"pct_point_change_funded_value\"], \"Percentage point change in funded value\"),\n",
    "        (\"pct_bad_debt_share_of_funded_value_baseline\", impact[\"pct_bad_debt_share_of_funded_value_baseline\"], \"Percent of funded value that is bad debt (baseline)\"),\n",
    "        (\"pct_bad_debt_share_of_funded_value_after_model\", impact[\"pct_bad_debt_share_of_funded_value_after_model\"], \"Percent of funded value that is bad debt (after model)\"),\n",
    "        (\"pct_point_change_bad_debt_share_of_funded_value\", impact[\"pct_point_change_bad_debt_share_of_funded_value\"], \"Percentage point change in bad-debt share of funded value\"),\n",
    "        (\"pct_good_debt_share_of_funded_value_baseline\", impact[\"pct_good_debt_share_of_funded_value_baseline\"], \"Percent of funded value that is good debt (baseline)\"),\n",
    "        (\"pct_of_funded_bad_value_correctly_flagged\", impact[\"pct_of_funded_bad_value_correctly_flagged\"], \"Percent of funded bad value correctly flagged by model\"),\n",
    "        (\"pct_of_funded_good_value_incorrectly_flagged\", impact[\"pct_of_funded_good_value_incorrectly_flagged\"], \"Percent of funded good value incorrectly flagged by model\"),\n",
    "        (\"pct_of_total_value_not_funded_but_predicted_good\", impact[\"pct_of_total_value_not_funded_but_predicted_good\"], \"Percent of total loan value not funded but predicted good\"),\n",
    "        (\"pct_profit_margin_baseline\", impact[\"pct_profit_margin_baseline\"], \"Profit margin (baseline)\"),\n",
    "        (\"pct_profit_margin_after_model\", impact[\"pct_profit_margin_after_model\"], \"Profit margin (after model)\"),\n",
    "        (\"pct_point_uplift_in_profit_margin\", impact[\"pct_point_uplift_in_profit_margin\"], \"Percentage point uplift in profit margin\"),\n",
    "    ]\n",
    "    pd.DataFrame(rows, columns=[\"Metric\", \"Value\", \"Description\"]).to_csv(analysis_dir / \"impact_table.csv\", index=False)\n",
    "\n",
    "    shutil.copy2(metrics_path, analysis_dir / \"metrics.json\")\n",
    "    shutil.copy2(cm_path, analysis_dir / \"confusion_matrix.json\")\n",
    "\n",
    "\n",
    "    print(\"=== Impact Summary ===\")\n",
    "    print(f\"Funded value baseline: {impact['pct_funded_value_baseline']:.2f}% of total\")\n",
    "    print(f\"Funded value after model: {impact['pct_funded_value_after_model']:.2f}% ({impact['pct_point_change_funded_value']:+.2f} pp)\")\n",
    "    print(f\"Bad-debt share of funded (baseline): {impact['pct_bad_debt_share_of_funded_value_baseline']:.2f}%\")\n",
    "    print(f\"Bad-debt share of funded (after): {impact['pct_bad_debt_share_of_funded_value_after_model']:.2f}% ({impact['pct_point_change_bad_debt_share_of_funded_value']:+.2f} pp)\")\n",
    "    print(f\"Bad funded correctly flagged: {impact['pct_of_funded_bad_value_correctly_flagged']:.2f}% of funded bad value\")\n",
    "    print(f\"Good funded incorrectly flagged: {impact['pct_of_funded_good_value_incorrectly_flagged']:.2f}% of funded value\")\n",
    "    print(f\"Not funded but predicted good: {impact['pct_of_total_value_not_funded_but_predicted_good']:.2f}% of total value\")\n",
    "    print(f\"Profit margin baseline: {impact['pct_profit_margin_baseline']:.2f}%\")\n",
    "    print(f\"Profit margin after model: {impact['pct_profit_margin_after_model']:.2f}% (uplift {impact['pct_point_uplift_in_profit_margin']:+.2f} pp)\")\n",
    "    print(f\"Wrote {out_json}\")\n",
    "    print(f\"Wrote {analysis_dir / 'impact_table.csv'}\")\n",
    "    print(f\"Copied {analysis_dir / 'metrics.json'} and {analysis_dir / 'confusion_matrix.json'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c7faa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charts generated:\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/cm_heatmap.png\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/profit_impact_bar.png\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/funded_value_change_bar.png\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/bad_good_share_pies.png\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/impact_changes_bar.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    _HAS_SNS = True\n",
    "except Exception:\n",
    "    _HAS_SNS = False\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _ensure_dir(path: Path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _load_json(path: Path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _cm_heatmap(cm: np.ndarray, out_path: Path):\n",
    "    plt.figure(figsize=(7, 6), dpi=150)\n",
    "    if _HAS_SNS:\n",
    "        sns.set_theme(style=\"white\")\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            cbar=False,\n",
    "            xticklabels=[\"Pred Good\", \"Pred Bad\"],\n",
    "            yticklabels=[\"Actual Good\", \"Actual Bad\"],\n",
    "            linewidths=0.5,\n",
    "            linecolor=\"white\",\n",
    "            annot_kws={\"fontsize\": 11, \"fontweight\": \"bold\"},\n",
    "        )\n",
    "    else:\n",
    "        ax = plt.gca()\n",
    "        im = ax.imshow(cm, cmap=\"Blues\")\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_xticklabels([\"Pred Good\", \"Pred Bad\"])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_yticklabels([\"Actual Good\", \"Actual Bad\"])\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, f\"{cm[i, j]:d}\", ha=\"center\", va=\"center\", color=\"black\", fontsize=11)\n",
    "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.title(\"Confusion Matrix\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _label_containers(ax, fmt=\"%.2f\"):\n",
    "    for container in ax.containers:\n",
    "        try:\n",
    "            ax.bar_label(container, fmt=fmt, padding=3, fontsize=10)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "def _bar_two_series(title, labels, values, colors, ylabel, out_path: Path):\n",
    "    plt.figure(figsize=(7.5, 5.5), dpi=150)\n",
    "    if _HAS_SNS:\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        ax = sns.barplot(x=labels, y=values, palette=colors)\n",
    "    else:\n",
    "        ax = plt.gca()\n",
    "        ax.bar(labels, values, color=colors)\n",
    "    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    _label_containers(ax, fmt=\"%.2f%%\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _bar_changes(title, metric_labels, pp_changes, colors, out_path: Path):\n",
    "    plt.figure(figsize=(8.5, 5.5), dpi=150)\n",
    "    if _HAS_SNS:\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        ax = sns.barplot(x=metric_labels, y=pp_changes, palette=colors)\n",
    "    else:\n",
    "        ax = plt.gca()\n",
    "        ax.bar(metric_labels, pp_changes, color=colors)\n",
    "    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Change (percentage points)\")\n",
    "    plt.axhline(0, color=\"#666\", linewidth=1)\n",
    "    _label_containers(ax, fmt=\"%.2f pp\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _two_pies(title_left, title_right, shares_left, shares_right, labels, colors, out_path: Path):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5.5), dpi=150)\n",
    "    axs[0].pie(\n",
    "        shares_left, labels=labels, autopct=\"%1.2f%%\", colors=colors, startangle=90, textprops={\"fontsize\": 10}\n",
    "    )\n",
    "    axs[0].set_title(title_left, fontsize=12, fontweight=\"bold\")\n",
    "    axs[1].pie(\n",
    "        shares_right, labels=labels, autopct=\"%1.2f%%\", colors=colors, startangle=90, textprops={\"fontsize\": 10}\n",
    "    )\n",
    "    axs[1].set_title(title_right, fontsize=12, fontweight=\"bold\")\n",
    "    plt.suptitle(\"Composition of Funded Value\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    repo = Path(\".\").resolve()\n",
    "    analysis_dir = repo / \"artifacts\" / \"analysis\"\n",
    "    _ensure_dir(analysis_dir)\n",
    "\n",
    "    impact_json = analysis_dir / \"impact_metrics.json\"\n",
    "    cm_json = analysis_dir / \"confusion_matrix.json\"\n",
    "    if not impact_json.exists():\n",
    "        raise FileNotFoundError(str(impact_json))\n",
    "    if not cm_json.exists():\n",
    "        raise FileNotFoundError(str(cm_json))\n",
    "\n",
    "    impact_all = _load_json(impact_json)\n",
    "    impact = impact_all.get(\"impact\", {})\n",
    "    cm_info = _load_json(cm_json)\n",
    "    cm = np.array(cm_info[\"matrix\"], dtype=int)\n",
    "\n",
    "    cm_path = analysis_dir / \"cm_heatmap.png\"\n",
    "    _cm_heatmap(cm, cm_path)\n",
    "\n",
    "    baseline_profit = float(impact.get(\"pct_profit_margin_baseline\", 0.0))\n",
    "    after_profit = float(impact.get(\"pct_profit_margin_after_model\", 0.0))\n",
    "    uplift_pp = float(impact.get(\"pct_point_uplift_in_profit_margin\", 0.0))\n",
    "    profit_path = analysis_dir / \"profit_impact_bar.png\"\n",
    "    _bar_two_series(\n",
    "        title=\"Profitability Impact\",\n",
    "        labels=[\"Baseline\", \"After Model\"],\n",
    "        values=[baseline_profit, after_profit],\n",
    "        colors=[\"#7cb342\", \"#1e88e5\"],\n",
    "        ylabel=\"Profit Margin (%)\",\n",
    "        out_path=profit_path,\n",
    "    )\n",
    "\n",
    "    funded_baseline = float(impact.get(\"pct_funded_value_baseline\", 0.0))\n",
    "    funded_after = float(impact.get(\"pct_funded_value_after_model\", 0.0))\n",
    "    funded_change_pp = float(impact.get(\"pct_point_change_funded_value\", 0.0))\n",
    "    funded_path = analysis_dir / \"funded_value_change_bar.png\"\n",
    "    _bar_two_series(\n",
    "        title=\"Funded Value (as % of Total)\",\n",
    "        labels=[\"Baseline\", \"After Model\"],\n",
    "        values=[funded_baseline, funded_after],\n",
    "        colors=[\"#6d4c41\", \"#26a69a\"],\n",
    "        ylabel=\"Percent of Total Value (%)\",\n",
    "        out_path=funded_path,\n",
    "    )\n",
    "\n",
    "    bad_share_baseline = float(impact.get(\"pct_bad_debt_share_of_funded_value_baseline\", 0.0))\n",
    "    bad_share_after = float(impact.get(\"pct_bad_debt_share_of_funded_value_after_model\", 0.0))\n",
    "    bad_change_pp = float(impact.get(\"pct_point_change_bad_debt_share_of_funded_value\", 0.0))\n",
    "    good_share_baseline = float(impact.get(\"pct_good_debt_share_of_funded_value_baseline\", max(0.0, 100.0 - bad_share_baseline)))\n",
    "    good_share_after = max(0.0, 100.0 - bad_share_after)\n",
    "    pies_path = analysis_dir / \"bad_good_share_pies.png\"\n",
    "    _two_pies(\n",
    "        title_left=\"Baseline\",\n",
    "        title_right=\"After Model\",\n",
    "        shares_left=[bad_share_baseline, good_share_baseline],\n",
    "        shares_right=[bad_share_after, good_share_after],\n",
    "        labels=[\"Bad Debt\", \"Good Debt\"],\n",
    "        colors=[\"#e53935\", \"#43a047\"],\n",
    "        out_path=pies_path,\n",
    "    )\n",
    "\n",
    "    changes_path = analysis_dir / \"impact_changes_bar.png\"\n",
    "    _bar_changes(\n",
    "        title=\"Key Impact Changes (percentage points)\",\n",
    "        metric_labels=[\"Funded Value\", \"Bad-Share of Funded\", \"Profit Margin\"],\n",
    "        pp_changes=[funded_change_pp, bad_change_pp, uplift_pp],\n",
    "        colors=[\"#8e24aa\", \"#fb8c00\", \"#3949ab\"],\n",
    "        out_path=changes_path,\n",
    "    )\n",
    "\n",
    "    print(\"Charts generated:\")\n",
    "    print(f\"- {cm_path}\")\n",
    "    print(f\"- {profit_path}\")\n",
    "    print(f\"- {funded_path}\")\n",
    "    print(f\"- {pies_path}\")\n",
    "    print(f\"- {changes_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0858dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagrams written:\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/diagrams/architecture.dot\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/diagrams/data_flow.dot\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/diagrams/architecture.png\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/diagrams/data_flow.png\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/diagrams/architecture.mmd\n",
      "- /Users/johnteng/Library/CloudStorage/GoogleDrive-ystengjohn@gmail.com/My Drive/DOC_ARCHIVE_2025-6/github/moneyLion-202511010/artifacts/analysis/diagrams/data_flow.mmd\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "try:\n",
    "    from graphviz import Source\n",
    "    _HAS_GRAPHVIZ = True\n",
    "except Exception:\n",
    "    _HAS_GRAPHVIZ = False\n",
    "\n",
    "\n",
    "def load_yaml(p: Path):\n",
    "    with open(p, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def write_text(p: Path, s: str):\n",
    "    p.write_text(s, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def build_architecture_dot(cfg: dict) -> str:\n",
    "    bucket = cfg[\"gcs_artifact_storage\"][\"bucket_name\"]\n",
    "    base_prefix = cfg[\"gcs_artifact_storage\"][\"base_prefix\"]\n",
    "    return f\"\"\"\n",
    "digraph ARCH {{\n",
    "  rankdir=LR;\n",
    "  graph [fontname=\"Helvetica\", fontsize=10];\n",
    "  node  [fontname=\"Helvetica\", fontsize=10, shape=box, style=rounded];\n",
    "  edge  [fontname=\"Helvetica\", fontsize=9];\n",
    "\n",
    "  subgraph cluster_orch {{\n",
    "    label=\"Orchestration (Composer / Airflow)\";\n",
    "    color=\"#90CAF9\";\n",
    "    DAG [label=\"DAG: moneylion_training_pipeline\"];\n",
    "    RUN [label=\"run_full_pipeline\\\\n(KubernetesPodOperator: python main.py)\"];\n",
    "    ROLL [label=\"rollout_cloud_run\\\\n(gcloud run services update)\"];\n",
    "    DAG -> RUN -> ROLL;\n",
    "  }}\n",
    "\n",
    "  subgraph cluster_raw {{\n",
    "    label=\"Raw Data (GCS)\";\n",
    "    color=\"#A5D6A7\";\n",
    "    RAW [label=\"gs://{bucket}/raw/\\\\nloan.csv, payment.csv, clarity_*.csv\", shape=folder];\n",
    "  }}\n",
    "\n",
    "  subgraph cluster_train {{\n",
    "    label=\"Training Pod (single run of main.py)\";\n",
    "    color=\"#FFE082\";\n",
    "    S1 [label=\"1. Data Ingestion\\\\n→ artifacts/data_ingestion/raw\"];\n",
    "    S2 [label=\"2. Transformation\\\\n→ joined_df.csv + stats/dummies\"];\n",
    "    S3 [label=\"3. Preprocessing\\\\n→ splits npy + vocabs\"];\n",
    "    S4 [label=\"4. Embedding & Export\\\\n→ embed_matrices + X_*.npy\"];\n",
    "    S5 [label=\"5. Model Training (XGB)\\\\n→ xgb_model.json + metrics + CM\"];\n",
    "    ANA [label=\"Analysis Scripts\\\\n→ predictions.csv + impact_metrics.json + charts/*.png\"];\n",
    "    S1 -> S2 -> S3 -> S4 -> S5 -> ANA;\n",
    "  }}\n",
    "\n",
    "  subgraph cluster_art {{\n",
    "    label=\"Artifacts (GCS)\";\n",
    "    color=\"#CE93D8\";\n",
    "    ART [label=\"gs://{bucket}/{base_prefix}/**\", shape=folder];\n",
    "  }}\n",
    "\n",
    "  subgraph cluster_srv {{\n",
    "    label=\"Serving (Cloud Run)\";\n",
    "    color=\"#EF9A9A\";\n",
    "    CR [label=\"Service: loan-default-api\"];\n",
    "    INIT [label=\"Startup: ensure_artifacts()\\\\nload model + schema\"];\n",
    "    API [label=\"API: /health, /predict\"];\n",
    "    CR -> INIT -> API;\n",
    "  }}\n",
    "\n",
    "  RAW -> S1 [label=\"download raw\"];\n",
    "  S1 -> ART [label=\"upload ingestion\"];\n",
    "  S2 -> ART [label=\"upload transformation\"];\n",
    "  S3 -> ART [label=\"upload preprocessing\"];\n",
    "  S4 -> ART [label=\"upload embedding\"];\n",
    "  S5 -> ART [label=\"upload model\"];\n",
    "  ANA -> ART [label=\"upload analysis\", style=dashed, color=\"#777\"];\n",
    "\n",
    "  ROLL -> CR [label=\"update env → new revision\"];\n",
    "  ART -> INIT [label=\"download on startup\"];\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_dataflow_dot(cfg: dict) -> str:\n",
    "    bucket = cfg[\"gcs_artifact_storage\"][\"bucket_name\"]\n",
    "    base_prefix = cfg[\"gcs_artifact_storage\"][\"base_prefix\"]\n",
    "    return f\"\"\"\n",
    "digraph FLOW {{\n",
    "  rankdir=TB;\n",
    "  node [shape=box, style=rounded, fontname=\"Helvetica\", fontsize=10];\n",
    "  edge [fontname=\"Helvetica\", fontsize=9];\n",
    "\n",
    "  RAW_L [label=\"gs://{bucket}/raw/loan.csv\", shape=folder];\n",
    "  RAW_P [label=\"gs://{bucket}/raw/payment.csv\", shape=folder];\n",
    "  RAW_C [label=\"gs://{bucket}/raw/clarity_underwriting_variables.csv\", shape=folder];\n",
    "\n",
    "  ING [label=\"artifacts/data_ingestion/raw/*.csv\"];\n",
    "  TRN [label=\"artifacts/data_transformation/\\\\njoined_df.csv + stats/dummies\"];\n",
    "  PRE [label=\"artifacts/data_preprocessing/\\\\ntrain|val|test {{num,cat,y}}.npy + vocabs.json\"];\n",
    "  EMB [label=\"artifacts/data_embedding/\\\\nembed_matrices/*.npy + X_*.npy + embed_schema.json\"];\n",
    "  MOD [label=\"artifacts/model_training/\\\\nxgb_model.json + metrics.json + confusion_matrix.json\"];\n",
    "  ANA [label=\"artifacts/analysis/\\\\npredictions.csv + impact_metrics.json + charts/*.png\"];\n",
    "\n",
    "  RAW_L -> ING; RAW_P -> ING; RAW_C -> ING;\n",
    "  ING -> TRN -> PRE -> EMB -> MOD -> ANA;\n",
    "\n",
    "  GCS [label=\"GCS mirror\\\\n(gs://{bucket}/{base_prefix}/**)\", shape=folder];\n",
    "  ING -> GCS; TRN -> GCS; PRE -> GCS; EMB -> GCS; MOD -> GCS; ANA -> GCS;\n",
    "\n",
    "  SRV [label=\"Cloud Run (loan-default-api)\"];\n",
    "  SRV -> MOD [label=\"startup download\", dir=back, style=dashed, color=\"#777\"];\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_mermaid_architecture(cfg: dict) -> str:\n",
    "    bucket = cfg[\"gcs_artifact_storage\"][\"bucket_name\"]\n",
    "    base_prefix = cfg[\"gcs_artifact_storage\"][\"base_prefix\"]\n",
    "    return f\"\"\"\n",
    "flowchart LR\n",
    "  subgraph Orchestration[Composer / Airflow]\n",
    "    DAG([DAG: moneylion_training_pipeline])\n",
    "    RUN([run_full_pipeline: python main.py])\n",
    "    ROLL([rollout_cloud_run: gcloud run update])\n",
    "    DAG --> RUN --> ROLL\n",
    "  end\n",
    "\n",
    "  subgraph Raw[GCS raw]\n",
    "    RAW[(gs://{bucket}/raw/*)]\n",
    "  end\n",
    "\n",
    "  subgraph Training[Training Pod]\n",
    "    S1([1. Ingestion → artifacts/data_ingestion/raw])\n",
    "    S2([2. Transformation → joined_df.csv + stats/dummies])\n",
    "    S3([3. Preprocessing → splits npy + vocabs])\n",
    "    S4([4. Embedding → embed_matrices + X_*.npy])\n",
    "    S5([5. Model Training (XGB) → xgb_model.json + metrics + CM])\n",
    "    ANA([Analysis → predictions.csv + impact_metrics.json + charts])\n",
    "    S1 --> S2 --> S3 --> S4 --> S5 --> ANA\n",
    "  end\n",
    "\n",
    "  subgraph Artifacts[GCS artifacts]\n",
    "    ART[/gs://{bucket}/{base_prefix}/**/]\n",
    "  end\n",
    "\n",
    "  subgraph Serving[Cloud Run]\n",
    "    CR[(loan-default-api)]\n",
    "    INIT([ensure_artifacts() at startup])\n",
    "    API[/GET /health, POST /predict/]\n",
    "    CR --> INIT --> API\n",
    "  end\n",
    "\n",
    "  RAW --> S1\n",
    "  RUN -. triggers .-> S1\n",
    "  S1 --> ART\n",
    "  S2 --> ART\n",
    "  S3 --> ART\n",
    "  S4 --> ART\n",
    "  S5 --> ART\n",
    "  ANA --> ART\n",
    "  ROLL --> CR\n",
    "  ART --> INIT\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_mermaid_dataflow(cfg: dict) -> str:\n",
    "    bucket = cfg[\"gcs_artifact_storage\"][\"bucket_name\"]\n",
    "    base_prefix = cfg[\"gcs_artifact_storage\"][\"base_prefix\"]\n",
    "    return f\"\"\"\n",
    "flowchart TB\n",
    "  RAW_L[(gs://{bucket}/raw/loan.csv)]\n",
    "  RAW_P[(gs://{bucket}/raw/payment.csv)]\n",
    "  RAW_C[(gs://{bucket}/raw/clarity_underwriting_variables.csv)]\n",
    "\n",
    "  ING[artifacts/data_ingestion/raw/*.csv]\n",
    "  TRN[artifacts/data_transformation/joined_df.csv + stats/dummies]\n",
    "  PRE[artifacts/data_preprocessing/ splits + vocabs]\n",
    "  EMB[artifacts/data_embedding/ embed_matrices + X_*.npy]\n",
    "  MOD[artifacts/model_training/ xgb_model.json + metrics + CM]\n",
    "  ANA[artifacts/analysis/ predictions.csv + impact_metrics.json + charts]\n",
    "\n",
    "  RAW_L --> ING\n",
    "  RAW_P --> ING\n",
    "  RAW_C --> ING\n",
    "  ING --> TRN --> PRE --> EMB --> MOD --> ANA\n",
    "\n",
    "  ART[/gs://{bucket}/{base_prefix}/**/]\n",
    "  ING --> ART\n",
    "  TRN --> ART\n",
    "  PRE --> ART\n",
    "  EMB --> ART\n",
    "  MOD --> ART\n",
    "  ANA --> ART\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def render_graphviz(dot_src: str, out_png: Path, out_dot: Path):\n",
    "    write_text(out_dot, dot_src)\n",
    "    if _HAS_GRAPHVIZ:\n",
    "        Source(dot_src, filename=str(out_png.with_suffix(\"\")), format=\"png\").render(cleanup=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    repo = Path(\".\").resolve()\n",
    "    cfg = load_yaml(repo / \"yamls\" / \"config.yaml\")\n",
    "\n",
    "    out_dir = repo / \"artifacts\" / \"analysis\" / \"diagrams\"\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    arch_dot = build_architecture_dot(cfg)\n",
    "    flow_dot = build_dataflow_dot(cfg)\n",
    "    render_graphviz(arch_dot, out_dir / \"architecture.png\", out_dir / \"architecture.dot\")\n",
    "    render_graphviz(flow_dot, out_dir / \"data_flow.png\", out_dir / \"data_flow.dot\")\n",
    "\n",
    "    arch_mmd = build_mermaid_architecture(cfg)\n",
    "    flow_mmd = build_mermaid_dataflow(cfg)\n",
    "    write_text(out_dir / \"architecture.mmd\", arch_mmd)\n",
    "    write_text(out_dir / \"data_flow.mmd\", flow_mmd)\n",
    "\n",
    "    print(\"Diagrams written:\")\n",
    "    print(f\"- {out_dir / 'architecture.dot'}\")\n",
    "    print(f\"- {out_dir / 'data_flow.dot'}\")\n",
    "    if _HAS_GRAPHVIZ:\n",
    "        print(f\"- {out_dir / 'architecture.png'}\")\n",
    "        print(f\"- {out_dir / 'data_flow.png'}\")\n",
    "    else:\n",
    "        print(\"Graphviz not available: PNGs not rendered; use the .dot or .mmd files in draw.io.\")\n",
    "    print(f\"- {out_dir / 'architecture.mmd'}\")\n",
    "    print(f\"- {out_dir / 'data_flow.mmd'}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
